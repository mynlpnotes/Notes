# Encoder Decoder

* This is not used now
* seq2seq models
* Cannot be trained parallelly, as RNN are time dependent
* This is overcome in BERT, GPT
* We use BERT and GPT now
