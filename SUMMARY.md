# Table of contents

* [Introduction](README.md)
  * [Introduction](Introduction/Introduction.md)
  * [NLP Tasks](introduction/nlp-tasks.md)
  * [Difficulty level](introduction/difficulty-level.md)
  * [Approaches to NLP](introduction/approaches-to-nlp.md)
  * [Why is NLP challening](introduction/why-is-nlp-challening.md)
  * [End to End NLP pipeline](introduction/end-to-end-nlp-pipeline.md)
  * [Real life example of PCA](introduction/real-life-example-of-pca.md)
* [Text Pre Processing](text-pre-processing/README.md)
  * [Tokenization](text-pre-processing/tokenization.md)
  * [Stopwords](text-pre-processing/stopwords.md)
  * [Stemming](text-pre-processing/stemming.md)
  * [Lemmatization](text-pre-processing/lemmatization.md)
  * [Basic Terminologies](text-pre-processing/basic-terminologies.md)
  * [Pre-Processing Steps](text-pre-processing/pre-processing-steps.md)
  * [N-grams](text-pre-processing/n-grams.md)
  * [Code](text-pre-processing/code.md)
* [Word Embeddings](word-embeddings/README.md)
  * [One Hot Encoding](word-embeddings/one-hot-encoding.md)
  * [Bag of words](word-embeddings/bag-of-words.md)
  * [TF-IDF](word-embeddings/tf-idf.md)
  * [Word2Vec](word-embeddings/word2vec.md)
  * [CBOW](word-embeddings/cbow.md)
  * [AvgWord2Vec](word-embeddings/avgword2vec.md)
  * [Skip Gram](word-embeddings/skip-gram.md)
  * [Code](word-embeddings/code.md)
  * [Label Encoder](word-embeddings/label-encoder.md)
  * [Letter Embedding](word-embeddings/letter-embedding.md)
  * [Letter Embedding- Code](word-embeddings/letter-embedding-code.md)
  * [Code - Text Generation](word-embeddings/code-text-generation.md)
  * [Code - Text Generation - Additional Explanation](word-embeddings/code-text-generation-additional-explanation.md)
* [Similarity Measures](similarity-measures/README.md)
  * [Jaccard Similarity](similarity-measures/jaccard-similarity.md)
  * [Euclidean Distance](similarity-measures/euclidean-distance.md)
  * [Cosine Similarity](similarity-measures/cosine-similarity.md)
* [RNN](rnn/README.md)
  * [RNN Overview](rnn/rnn-overview.md)
  * [Example of Sequential Memory](rnn/example-of-sequential-memory.md)
  * [Short term memory problem](rnn/short-term-memory-problem.md)
  * [Data passing to RNN](rnn/data-passing-to-rnn.md)
  * [Types of RNN](rnn/types-of-rnn.md)
  * [Different Configurations](rnn/different-configurations.md)
  * [Forward Propagation](rnn/forward-propagation.md)
  * [Back Propagation](rnn/back-propagation.md)
  * [Vanishing Gradient Problem](rnn/vanishing-gradient-problem.md)
  * [Equations](rnn/equations.md)
  * [Code - Classifier](rnn/code-classifier.md)
* [Bi RNN](bi-rnn/README.md)
  * [Overview](bi-rnn/overview.md)
  * [Code - Sentiment Analysis](bi-rnn/code-sentiment-analysis.md)
* [LSTM](lstm/README.md)
  * [Overview](lstm/overview.md)
  * [What is LSTM](lstm/what-is-lstm.md)
  * [Article - Understanding LSTM](lstm/article-understanding-lstm.md)
  * [LSTM](lstm/lstm.md)
  * [Architecture](lstm/architecture.md)
  * [Equations](lstm/equations.md)
  * [Behavior of network over time](lstm/behavior-of-network-over-time.md)
  * [Stacking](lstm/stacking.md)
* [GRU](gru/README.md)
  * [Overview](gru/overview.md)
  * [Main Simplification](gru/main-simplification.md)
  * [Equations](gru/equations.md)
  * [Advantage/Disadvantage](gru/advantage-disadvantage.md)
* [Encoder Decoder](encoder-decoder/README.md)
  * [Neural Machine Translation](encoder-decoder/neural-machine-translation.md)
  * [Sequence Length](encoder-decoder/sequence-length.md)
  * [Softmax](encoder-decoder/softmax.md)
  * [Prediction](encoder-decoder/prediction.md)
  * [Embedding Layer](encoder-decoder/embedding-layer.md)
* [Attention Mechanism](attention-mechanism/README.md)
  * [Introduction](attention-mechanism/introduction.md)
  * [Key Points](attention-mechanism/key-points.md)
  * [Self Attention](attention-mechanism/self-attention.md)
  * [Attention is all you need](attention-mechanism/attention-is-all-you-need.md)
* [Interview Questions](interview-questions.md)
* [Transformers](transformers/README.md)
  * [Hugging face overview](transformers/hugging-face-overview.md)
  * [Basics](transformers/basics.md)
  * [Biasness and Limitations of Transformers](transformers/biasness-and-limitations-of-transformers.md)
  * [Different types of Transformer Models](transformers/different-types-of-transformer-models.md)
  * [Pipelines](transformers/pipelines.md)
  * [Models](transformers/models.md)
  * [Tokenizer](transformers/tokenizer.md)
  * [Pre Processing and Training](transformers/pre-processing-and-training.md)
  * [Login and Sharing](transformers/login-and-sharing.md)
* [DialogFlow](dialogflow/README.md)
  * [Chatbot Overview](dialogflow/chatbot-overview.md)
  * [Types of chatbots](dialogflow/types-of-chatbots.md)
  * [Use of chatbot](dialogflow/use-of-chatbot.md)
  * [Examples of chatbot](dialogflow/examples-of-chatbot.md)
  * [What is Google Dialogflow](dialogflow/what-is-google-dialogflow.md)
  * [Creating Account](dialogflow/creating-account.md)

## Introduction to Transformer Models for NLP

* [Lesson 1: Introduction to Attention and Language Models](introduction-to-transformer-models-for-nlp/lesson-1-introduction-to-attention-and-language-models/README.md)
  * [1.1 A Brief History of NLP](introduction-to-transformer-models-for-nlp/lesson-1-introduction-to-attention-and-language-models/1.1-a-brief-history-of-nlp.md)
  * [1.2 Pay Attention with Attention](introduction-to-transformer-models-for-nlp/lesson-1-introduction-to-attention-and-language-models/1.2-pay-attention-with-attention.md)
  * [1.3 Encoder and Decoder Architecture](introduction-to-transformer-models-for-nlp/lesson-1-introduction-to-attention-and-language-models/1.3-encoder-and-decoder-architecture.md)
  * [1.4 How Language Model Looks at text](introduction-to-transformer-models-for-nlp/lesson-1-introduction-to-attention-and-language-models/1.4-how-language-model-looks-at-text.md)
* [Lesson 2: How transformers use attention to process text](introduction-to-transformer-models-for-nlp/lesson-2-how-transformers-use-attention-to-process-text/README.md)
  * [2.1 Introduction to Transformers](introduction-to-transformer-models-for-nlp/lesson-2-how-transformers-use-attention-to-process-text/2.1-introduction-to-transformers.md)
  * [2.2 Scaled Dot Product Attention](introduction-to-transformer-models-for-nlp/lesson-2-how-transformers-use-attention-to-process-text/2.2-scaled-dot-product-attention.md)
  * [2.3 Multi Headed Attention](introduction-to-transformer-models-for-nlp/lesson-2-how-transformers-use-attention-to-process-text/2.3-multi-headed-attention.md)
* [Lesson 3: Transfer Learning](introduction-to-transformer-models-for-nlp/lesson-3-transfer-learning/README.md)
  * [3.1 Introduction to Transfer Learning](introduction-to-transformer-models-for-nlp/lesson-3-transfer-learning/3.1-introduction-to-transfer-learning.md)
  * [3.2 Introduction to PyTorch](introduction-to-transformer-models-for-nlp/lesson-3-transfer-learning/3.2-introduction-to-pytorch.md)
  * [3.3 Fine tuning Transformers with PyTorch](introduction-to-transformer-models-for-nlp/lesson-3-transfer-learning/3.3-fine-tuning-transformers-with-pytorch.md)
* [Lesson 4:NLU with BERT](introduction-to-transformer-models-for-nlp/lesson-4-nlu-with-bert/README.md)
  * [4.1 Introduction to BERT](introduction-to-transformer-models-for-nlp/lesson-4-nlu-with-bert/4.1-introduction-to-bert.md)
  * [4.2 Wordpiece Tokenization](introduction-to-transformer-models-for-nlp/lesson-4-nlu-with-bert/4.2-wordpiece-tokenization.md)
  * [4.3 The many embeddings of BERT](introduction-to-transformer-models-for-nlp/lesson-4-nlu-with-bert/4.3-the-many-embeddings-of-bert.md)
* [Lesson 5: Pre-Tuning and Fine-Tuning BERT](introduction-to-transformer-models-for-nlp/lesson-5-pre-tuning-and-fine-tuning-bert/README.md)
  * [5.1 The Masked Language Modelling Task](introduction-to-transformer-models-for-nlp/lesson-5-pre-tuning-and-fine-tuning-bert/5.1-the-masked-language-modelling-task.md)
  * [5.2 Next Sentence Prediction Task](introduction-to-transformer-models-for-nlp/lesson-5-pre-tuning-and-fine-tuning-bert/5.2-next-sentence-prediction-task.md)
  * [5.3 Fine Tuning BERT to solve NLP Problems](introduction-to-transformer-models-for-nlp/lesson-5-pre-tuning-and-fine-tuning-bert/5.3-fine-tuning-bert-to-solve-nlp-problems.md)
* [Lesson 6: Hand-on BERT](introduction-to-transformer-models-for-nlp/lesson-6-hand-on-bert/README.md)
  * [6.1 Flavors of BERT](introduction-to-transformer-models-for-nlp/lesson-6-hand-on-bert/6.1-flavors-of-bert.md)
  * [6.2 BERT for Sequence Classification](introduction-to-transformer-models-for-nlp/lesson-6-hand-on-bert/6.2-bert-for-sequence-classification.md)
  * [6.3 BERT for Token Classification](introduction-to-transformer-models-for-nlp/lesson-6-hand-on-bert/6.3-bert-for-token-classification.md)
  * [6.4 BERT for QA](introduction-to-transformer-models-for-nlp/lesson-6-hand-on-bert/6.4-bert-for-qa.md)
