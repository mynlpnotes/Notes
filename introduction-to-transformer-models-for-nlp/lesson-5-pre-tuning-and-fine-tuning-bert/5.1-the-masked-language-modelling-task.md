# 5.1 The Masked Language Modelling Task

Tasks that are used to learn general language rules like pronoun antecedent and direct object verb relations

***

**Pre-training:**

* These tasks are generally not useful tasks but they help BERT learn how words/language work in general.

1. The masked language model
2. Next sentence prediction

&#x20;

MLM:

\-        Replace 15% of words in corpus with special \[MASK] token and ask BERT to fill in the blanks

\-        Think back to our \_\_\_ at the light – This is the MLM task

\-        Once we replace the words in our corpora with the MASK token, we then ask BERT to fill on the blank

\-        Istanbul is a great \[MASK] to visit

\-        When BERT is asked to perform this task, we give it a sentence or a group of sentences, and BERT will have to guess the word

\-        ![](file:///C:/Users/hites/AppData/Local/Packages/oice\_16\_974fa576\_32c1d314\_2990/AC/Temp/msohtmlclip1/01/clip\_image002.png)

\-        Once we pass the sentence through BERT, every single embedding has been imbued with context

\-        So every token including our MASK token is now context-full.

\-        We add on a feedforward layer on top of the context-full representation of the mask token and then basically it performs a classification task

&#x20;

Code:

\-        5. Pretuning\_finetuning BERT

\-        # BERTForMaskedLM is predefined class in the transformer library, that is specifically designed to add the necessary layers

\-        # for MLM task

\-        from transformers import BertForMaskedLM, pipeline

&#x20;

\-        # Pipelines in transformers take in models/tokenizers and are easy way to perform several tasks

\-        # We can perform an auto-encoder language model task

\-        nlp = pipeline("fill-mask", model='bert-base-cased')  # could also do "model=bert\_lm" for the same result

&#x20;

\-        preds = nlp(f"If you don’t {nlp.tokenizer.mask\_token} at the sign, you will get a ticket.")

\-        for p in preds:

\-            print(f"Token:{p\['token\_str']}. Score: {100\*p\['score']:,.2f}%")

\-         If you don’t \*\*\* at the sign, you will get a ticket.

\-         Token:look. Score: 48.00%

\-         Token:stop. Score: 42.63%

\-         Token:glance. Score: 1.39%

\-         Token:arrive. Score: 0.88%

\-         Token:turn. Score: 0.62%

&#x20;

&#x20;

&#x20;

&#x20;
