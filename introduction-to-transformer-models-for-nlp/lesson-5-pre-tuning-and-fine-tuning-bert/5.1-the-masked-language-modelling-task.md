# 5.1 The Masked Language Modelling Task

Tasks that are used to learn general language rules like pronoun antecedent and direct object verb relations

***

**Pre-training:**

* These tasks are generally not useful tasks but they help BERT learn how words/language work in general.

1. The masked language model
2. Next sentence prediction

***

**MLM:**

* Replace 15% of words in corpus with special \[MASK] token and ask BERT to fill in the blanks
* Think back to our \_\_\_ at the light – This is the MLM task
* Once we replace the words in our corpora with the MASK token, we then ask BERT to fill on the blank
* Istanbul is a great \[MASK] to visit
* When BERT is asked to perform this task, we give it a sentence or a group of sentences, and BERT will have to guess the word
*

    <figure><img src="../../.gitbook/assets/image.png" alt=""><figcaption></figcaption></figure>
* Once we pass the sentence through BERT, every single embedding has been imbued with context
* So every token including our MASK token is now context-full.
* We add on a feedforward layer on top of the context-full representation of the mask token and then basically it performs a classification task

***

```python
# BERTForMaskedLM is predefined class in the transformer library, 
# that is specifically designed to add the necessary layers
# for MLM task
from transformers import BertForMaskedLM, pipeline
 
# Pipelines in transformers take in models/tokenizers and are easy way to 
# perform several tasks
# We can perform an auto-encoder language model task
nlp = pipeline("fill-mask", model='bert-base-cased')  
# could also do "model=bert_lm" for the same result
 
preds = nlp(f"If you don’t {nlp.tokenizer.mask_token} at the sign, you will get a ticket.")
for p in preds:
    print(f"Token:{p['token_str']}. Score: {100*p['score']:,.2f}%")
# Output
# If you don’t *** at the sign, you will get a ticket.
# Token:look. Score: 48.00%
# Token:stop. Score: 42.63%
# Token:glance. Score: 1.39%
# Token:arrive. Score: 0.88%
# Token:turn. Score: 0.62%
```

&#x20;

&#x20;

&#x20;

&#x20;
