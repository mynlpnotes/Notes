# 5.2 Next Sentence Prediction Task

**Next sentence prediction:**

* Classification problem
* Give 2 sentences, did sentence B come directly after sentence A? True or False
*

    <figure><img src="../../.gitbook/assets/image (1).png" alt=""><figcaption></figcaption></figure>
*

    <figure><img src="../../.gitbook/assets/image (2).png" alt=""><figcaption></figcaption></figure>
* Check the placement of \[SEP] token
* The pooler layer sits on top of the \[CLS] token and is used to process the context-ful \[CLS] representation for use in downstream task
* This is how a \[CLS] token is trained to become a representation of the entire sequence in BERT
* \[CLS] token is then feed through a feed forward and softmax layer pooler

***

```python
from transformers import BertForNextSentencePrediction, BertTokenizer

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
bert_nsp = BertForNextSentencePrediction.from_pretrained('bert-base-uncased')

text = "Deliver huge improvements.....hours fine-tuning parameters!"
text2 = "This bookâ€™s practical ..... ML results."

inputs = tokenizer(text, text2, return_tensors='pt')

# 0 == "isNextSentence" and 1 == "notNextSentence"
outputs = bert_nsp(**inputs)
NextSentencePredictorOutput(loss=None, logits=tensor([[ 6.0295, -5.5733]], 
           grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)

```
