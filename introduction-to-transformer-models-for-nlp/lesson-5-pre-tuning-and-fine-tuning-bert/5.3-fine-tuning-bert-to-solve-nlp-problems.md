# 5.3 Fine Tuning BERT to solve NLP Problems

Once BERT has general idea about how MLM and NSP, we can use what BERT has learned and fine-tune it on a specific task of our choosing

**Classification:**

*

    <figure><img src="../../.gitbook/assets/image (3) (1).png" alt=""><figcaption></figcaption></figure>
* Here we will be feeding it only a single sentence
* We are going to ignore the representation of every token apart from CLS
* Feed forward layer will output outcome

***

**Token classification:**

* Here we do care about the representation of every single token after passing our sentence through BERT
* This representation will be passed through a feed forward layer to classify each token
* NER for example where we are trying to identify different entities and for this we need to classify every single token with a label
*   &#x20;

    <figure><img src="../../.gitbook/assets/image (4).png" alt=""><figcaption></figcaption></figure>

***

**QA:**    &#x20;

*   &#x20; &#x20;

    <figure><img src="../../.gitbook/assets/image (5).png" alt=""><figcaption></figcaption></figure>
* We will be taking 2 sequences, question and context(some piece of text, which has the answer within it)
* We are going to add a layer on top of every single token
* And we are predicting whether or not specific token represents the start or end of the answer to the question
* So when we output our labels for each of then tokens, every single token is going to be given a probability of whether or not it’s the start of the answer or the end of the answer
* When we know which token is the start and which token is the end, we then take every token in between as the answer to our question

***

```python
from transformers import pipeline, BertForQuestionAnswering, BertForTokenClassification, BertForSequenceClassification

# Sequence Classification
bert_sq = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3)
finbert = pipeline('text-classification', model='ProsusAI/finbert', tokenizer='ProsusAI/finbert')
finbert('Stocks rallied and the British pound gained')
# Output: [{'label': 'positive', 'score': 0.6949423551559448}]

# Token classification
bert_tc = BertForTokenClassification.from_pretrained('bert-base-uncased')
custom_module = 'savasy/bert-base-turkish-ner-cased'
ner=pipeline('ner', model=custom_module, tokenizer=custom_module)
sequence = "Merhaba! Benim adım Sinan. San Francisco'dan geliyorum" # Hi! I'm Sinan. I come from San Francisco"
ner(sequence)
# Output:
#[{'entity': 'B-PER', 'score': 0.72424734,'index': 5,'word': 'Sinan',
#                'start': 20 'end': 25},
# {'entity': 'B-LOC', 'score': 0.99879956,'index': 7,'word': 'San',
#        	  'start': 27,'end': 30},.....}]


# QA
bert_qa = BertForQuestionAnswering.from_pretrained('bert-base-uncased')
model_name = "deepset/roberta-base-squad2"
qa = pipeline(model=model_name, tokenizer=model_name, revision="v1.0", task="question-answering")
sequence = "Where is Sinan living these days?", 
    "Sinan lives in California but Matt lives in Boston."
qa(*sequence)
# Output: {'score': 0.980839729309082, 'start': 15, 'end': 25, 'answer': 'California'}

```

&#x20;

&#x20;

&#x20;
