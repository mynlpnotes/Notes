# 4.2 Wordpiece Tokenization

* Consider the following sentence – “Another beautiful day”
* To tokenize this, we split into a list of tokens in our vocabulary over 30k tokens
* We also add two special tokens \[CLS] at the beginning of the phrase, and \[SEP] at the end of the phrase
* \[“\[CLS]”,”Another” ,”beautiful” ,”day” ,”\[SEP]”]
* BERT’s tokenizer is great at handling tokens that are OOV by breaking them up into smaller chunks of known tokens
* Sinan is oov
* \[“\[CLS]”,”sin”,”##an”,”a” ,”beautiful” ,”day” ,”\[SEP]”]
* \## indicates a subword
* Here we see that one word becomes 2 tokens in BERT
* Because BERT doesn’t know every single word in any language, it has to break up plenty of words into sub words
* If we give it data of a particular domain then it has to learn these different sub words context, given everything else
* When freezing the word embeddings for the sub words, we have to be careful because then BERT will never pick up on the specific usage of those tokens in the greater domains context

***

**Note on tokenization:**

* BERT has a maximum sequence length of 512 tokens. This was implemented for the sake of efficiency
* Any sequence less than 512 tokens it will be padded to reach 512 and if it is greater than 512 the model may error out

***

**Tokenization – Uncased vs Cased:**

1. **Uncased:**

* Removes accents
* Lower case the input
* Café dupont à cafe dupont

2. **Cased:**

* Does nothing to the input

***

* Uncased tokenization is usually better for most situations because case generally doesn’t contribute to context
* Cased tokenization works well in cases where case doesn’t matter (like NER)
* Note that this has little to do with the BERT architecture, just in tokenization

***

**Representation of words with context:**

* Consider the following sentences:
* I love my pet python
* I love coding in python
* \[‘\[CLS]’,’I’, ’love’ ,’my’ ,’pet’ ,’python’ ,’\[SEP]’]
* The vector representation for python will be different for each sentence because of the surrounding words in the sentence

```python
text = "A simple sentence!"
tokens = tokenizer.encode(text)  # get token ids per BERT-base's vocabulary
# Output: [101, 1037, 3722, 6251, 999, 102]

# python is the 6th token (don't forget the [CLS] token!)
python_pet = tokenizer.encode('I love my pet python'
python_language = tokenizer.encode('I love coding in python')
python_pet_embedding = model(torch.tensor(python_pet).unsqueeze(0))[0][:,5,:].detach().numpy()
python_language_embedding = model(torch.tensor(python_language).unsqueeze(0))[0][:,5,:].detach().numpy()
snake_alone_embedding = model(torch.tensor(tokenizer.encode('snake')).unsqueeze(0))[0][:,1,:].detach().numpy()
programming_alone_embedding = model(torch.tensor(tokenizer.encode('programming')).unsqueeze(0))[0][:,1,:].detach().numpy()

cosine_similarity(python_language_embedding, snake_alone_embedding) 
# Output: 0.5
cosine_similarity(python_pet_embedding, snake_alone_embedding)
# Output: 0.6

tokenizer(text)  # calling the tokenizer directly does the same thing as encode_plus
# Output:
# {'input_ids': [101, 2026, 2767, 2409, 2033, 2055, 2023, 2465, 1998, 1045, 2293,
#  2009, 2061, 2521, 999, 2016, 2001, 2157, 1012, 102], 
# 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
# 0, 0, 0, 0, 0, 0], 
# 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1
# , 1, 1, 1, 1, 1, 1]}
# Token_type_is: If we pass only 1 sequence then it will be 0, 
# if we pass 2 sequences, then it will be 0 and 1
# If we want to give attention then attention mask will be 1, 
# otherwise it will be 0

```
