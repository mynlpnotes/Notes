# 4.3 The many embeddings of BERT

**BERT applies 3 separate types of embeddings to tokenize sentences:**

1. **Token embeddings:**

* Represents context less meaning of each token
* A lookup of 30,522 possible vectors (for BERT-base)
* This is learnable during training

2. **Segment embedding:**

* Distinguishes between multiples inputs (For QA for example)
* A lookup of 2 possible vectors (One for sentence A one for sentence B)
* This is not learnable

3. **Position embeddings:**

* Used to represent the tokens position in the sentence
* This is not learnable
*

    <figure><img src="../../.gitbook/assets/image (5) (1).png" alt=""><figcaption></figcaption></figure>

***

**Position embedding:**

*

    <figure><img src="../../.gitbook/assets/image (6) (1).png" alt=""><figcaption></figcaption></figure>

\-       &#x20;

<figure><img src="../../.gitbook/assets/image (7).png" alt=""><figcaption></figcaption></figure>

* This formula provides a unique value for positions of tokens and positions in the embedding dimension

***

**BERT visualizer:**

<figure><img src="../../.gitbook/assets/image (8).png" alt=""><figcaption></figcaption></figure>

*

```python
example_phrase = 'I am Sinan'

# return_tensors='pt' converts to pytorch automatically
tokenizer.encode(example_phrase, return_tensors='pt')
# Output: tensor([[ 101, 1045, 2572, 8254, 2319,  102]])

model.embeddings.word_embeddings(tokenizer.encode(example_phrase, return_tensors='pt'))
# Output:	
#tensor([[[ 0.0136, -0.0265, -0.0235,  ...,  0.0087,  0.0071,  0.0151],
#	         [-0.0211,  0.0059, -0.0179,  ...,  0.0163,  0.0122,  0.0073],
#	         [-0.0437, -0.0150,  0.0029,  ..., -0.0282,  0.0474, -0.0448],
#	         [-0.0022, -0.0876,  0.0143,  ...,  0.0232, -0.0024, -0.0213],
#	         [-0.0614, -0.0044, -0.0755,  ..., -0.0522, -0.0310, -0.0248],
#	         [-0.0145, -0.0100,  0.0060,  ..., -0.0250,  0.0046, -0.0015]]],
#	       grad_fn=<EmbeddingBackward0>)

model.embeddings.position_embeddings(torch.LongTensor(range(6)))  # positional embeddings for our example_phrase
model.embeddings.token_type_embeddings(torch.LongTensor([0]*6))  # All tokens have the same embedding

# To get the final embedding:
# Apply feed forward normalization layer
model.embeddings.LayerNorm(
    model.embeddings.word_embeddings(tokenizer.encode(example_phrase, return_tensors='pt')) + \
    model.embeddings.position_embeddings(torch.LongTensor(range(6))) + \
    model.embeddings.token_type_embeddings(torch.LongTensor([0]*6))
)
model.embeddings(tokenizer.encode(example_phrase, return_tensors='pt'))

# Both give the same output
model.embeddings(tokenizer.encode(example_phrase, return_tensors='pt')).shape
# Output: torch.Size([1, 6, 768])
# 1 batch of 6 tokens, with each token of embedding 768
```
