# 4.1 Introduction to BERT

**BERT:**

* Bi-directional Encoder Representation from Transformers
* Auto encoding language model
* Uses only encoder from the Transformer
* Relying on self attention
* The encoder is taken from the transformer architecture

***

* Consider the following sentence: ‘I love my pet python’
* We feed this sentence into BERT to get a context-ful representation(vector embedding) of every word in the sentence
* The encoder understands the context of each word in the sentence using a multi-head attention mechanism (which relates each word to every other word in the sentence)
*

    <figure><img src="../../.gitbook/assets/image (6).png" alt=""><figcaption></figcaption></figure>
* Here we feed it a sequence of tokens through the encoder stack
* The output of BERT is a representation of every single token in the sequence
* \[CLS] – Special reserved token, specially for BERT, put at the beginning of every single sequence, and its goal is to represent the entire input
* This token will be used for doing tasks such as sequence classification
* \[SEP] is put at the end, which stands for separator is meant to represent a separation between 2 sequences
* Because BERT is able to take in either one or two sequences at a time depending on the task
* We pass only 1 sequences for classification
* We pass 2 sequences for task such as question and answering, where sequences 1 is question and sequence 2 is answer
* Each box at the top represents the vector representation of every token

***

**BERT size:**

*

    <figure><img src="../../.gitbook/assets/image (1) (1).png" alt=""><figcaption></figcaption></figure>
* BERT base has 12 encoder
* BERT small has 4 encoder
* BERT large has 24 encoder

***

*

    <figure><img src="../../.gitbook/assets/image (4) (1).png" alt=""><figcaption></figcaption></figure>
* Before the input reaches the Encoder, we need to tokenize it
* A token sometimes does not represent an entire word and one word may not be represented by a single token
* Each encoder is a combination of multi headed attention and add norm feedforward network, this promotes generalizability and prevent overfitting and also speed up training

```python
model = BertModel.from_pretrained('bert-base-uncased')

-            The BERT model has 199 different named parameters.
-             
-            ==== Embedding Layer ====
-             
-            embeddings.word_embeddings.weight                       (30522, 768)
-            embeddings.position_embeddings.weight                     (512, 768)
-            embeddings.token_type_embeddings.weight                     (2, 768)
-            embeddings.LayerNorm.weight                                   (768,)
-            embeddings.LayerNorm.bias                                     (768,)
-             
-            ==== First Encoder ====
-             
-            encoder.layer.0.attention.self.query.weight               (768, 768)
-            encoder.layer.0.attention.self.query.bias                     (768,)
-            encoder.layer.0.attention.self.key.weight                 (768, 768)
-            encoder.layer.0.attention.self.key.bias                       (768,)
-            encoder.layer.0.attention.self.value.weight               (768, 768)
-            encoder.layer.0.attention.self.value.bias                     (768,)
-            encoder.layer.0.attention.output.dense.weight             (768, 768)
-            encoder.layer.0.attention.output.dense.bias                   (768,)
-            encoder.layer.0.attention.output.LayerNorm.weight             (768,)
-            encoder.layer.0.attention.output.LayerNorm.bias               (768,)
-            encoder.layer.0.intermediate.dense.weight                (3072, 768)
-            encoder.layer.0.intermediate.dense.bias                      (3072,)
-            encoder.layer.0.output.dense.weight                      (768, 3072)
-            encoder.layer.0.output.dense.bias                             (768,)
-            encoder.layer.0.output.LayerNorm.weight                       (768,)
-            encoder.layer.0.output.LayerNorm.bias                         (768,)
-             
-            ==== Output Layer ====
-             
-            pooler.dense.weight                                       (768, 768)
-            pooler.dense.bias                                             (768,)
-             

#Embedding layer:
# BERT is aware of 30522 tokens
# Each of these tokens has a contextless embedding of 768
# Output layer has a pooler
# Pooler is a separate feed-forward network with a hyperbolic tangent 
# activation function that acts on [CLS] token representation
# When we are using BERT, the pooler takes the embedding of [CLS] token, 
# the one that is supposed to represent the entire sequence and it is adding a 
# layer on top of it, so that we can use the output of the pooler as a vector 
# representation of the entire sequence
# The pooler acts a representation for the entire input sequence
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
tokenizer.encode('Sinan loves a beautiful day')  # tokenize a simple sequence
#Output:
# [101, 8254, 2319, 7459, 1037, 3376, 2154, 102]
# 101 is for [CLS] and 102 is for [SEP]
 
# run tokens through the model
#1 Turn tokens_with_unknown_words into a tensor (will be size (8,))
#2 Unsqueeze a first dimension to simulate batches. Resulting shape is (1, 8)
response = model(torch.tensor(tokenizer.encode('Sinan loves a beautiful day')).unsqueeze(0))
# When we pass it through BERT, we get many outputs
# Here the last hidden state, is the output of the 12th encoder
# Here the pooler output is to be used as a vector that represents the entire 
# sequence as a whole
```

\- &#x20;
