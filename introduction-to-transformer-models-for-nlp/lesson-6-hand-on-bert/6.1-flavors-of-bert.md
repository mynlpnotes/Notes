# 6.1 Flavors of BERT

BERT inspired several derivative architectures, each with their own specialties/ drawbacks

1. RoBERTa
2. DistilBERT
3. ALBERT

* Each flavor attempts to enhance BERT by altering its architecture and/or its training

***

**RoBERTa:**&#x20;

* Robustly optimized BERT approach

1. Author claims BERT was vastly under-trained
2. 10x the training data(16GB à160GB)
3. 15% more parameters
4. Removed the next sentence prediction task(training)

a. Claim is that the task is not useful

5. Dynamic masking patter à 4X the masking tasks to learn from(training)

*

    <figure><img src="../../.gitbook/assets/image.png" alt=""><figcaption></figcaption></figure>

***

**DistilBERT:**

1. Distillation is a technique to train a “student” model to replicate a “teacher” model
2. 40% fewer parameters, 60% faster

a. 97% of BERTs performance

3. Training via knowledge distillation (training)

*

    <figure><img src="../../.gitbook/assets/image (1).png" alt=""><figcaption></figcaption></figure>

***

**ALBERT: A lite BERT**

1. Optimize model performance/number of parameters(90% fewer parameters)
2. Factorized embedding parametrization.

a. Factorize token embedding to make them much smaller(by doing some dimension reduction)

3. Cross-layer parameter sharing(architecture)

a. Share parameters across layers (Not all but some of the parameters are shared across the encoder)

4. NSP task became the sentence order prediction task (training)

a. Theory is that NLP and MLM are too similar of a task. SOP is harder

b. Swap the order and use that as a negative example

*

    <figure><img src="../../.gitbook/assets/image (2).png" alt=""><figcaption></figcaption></figure>

***

```python
nlp = pipeline("fill-mask", model='bert-base-cased')
nlp = pipeline("fill-mask", model='roberta-base')
nlp = pipeline("fill-mask", model='distilroberta-base')
nlp = pipeline("fill-mask", model='distilbert-base-cased')
 
preds = nlp(f"If you don’t {nlp.tokenizer.mask_token} at the sign, you will get a ticket")
print('If you don’t *** at the sign, you will get a ticket')
for p in preds:
    print(f"Token:{p['token_str']}. Score: {100*p['score']:,.2f}%")
 
```
