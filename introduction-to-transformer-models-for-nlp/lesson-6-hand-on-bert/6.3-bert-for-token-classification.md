# 6.3 BERT for Token Classification

```python
# The given "token_labels" may not match up with the BERT wordpiece tokenization so
#  this function will map them to the tokenization that BERT uses
#  -100 is a reserved for labels where we do not want to calculate losses so BERT doesn't waste time
#  trying to predict tokens like CLS or SEP
	
def tokenize_and_align_labels(examples):
    tokenized_inputs = tokenizer(examples["tokens"], truncation=True, is_split_into_words=True)
    labels = []
    for i, label in enumerate(examples[f"token_labels"]):
        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to their respective word.
        previous_word_idx = None
        label_ids = []
        for word_idx in word_ids:
            if word_idx is None:  # Set the special tokens to -100.
                label_ids.append(-100)
            elif word_idx != previous_word_idx:  # Only label the first token of a given word.
                label_ids.append(label[word_idx])
            else:
                label_ids.append(-100)  # CLS and SEP are labeled as -100
            previous_word_idx = word_idx
            labels.append(label_ids)
	
	tokenized_inputs["labels"] = labels
        return tokenized_inputs

tok_clf_tokenized_snips['train'] = tok_clf_tokenized_snips['train'].remove_columns(
['utterance', 'label', 'tokens', 'token_labels'] )
tok_clf_tokenized_snips['test'] = tok_clf_tokenized_snips['test'].remove_columns(
['utterance', 'label', 'tokens', 'token_labels'])
tok_clf_tokenized_snips
# Output:
# DatasetDict({
# train: Dataset({
# features: ['input_ids', 'attention_mask', 'labels'],
# num_rows: 10467
# })
# test: Dataset({
# features: ['input_ids', 'attention_mask', 'labels'],
# num_rows: 2617
# })
# })

tok_data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)
tok_clf_model = DistilBertForTokenClassification.from_pretrained(
	    'distilbert-base-cased', num_labels=len(unique_token_labels)
	)

tok_clf_model.config.id2label = {i: l for i, l in enumerate(unique_token_labels)}
tok_clf_model.config.id2label[0], tok_clf_model.config.id2label[1]

epochs = 2
training_args = TrainingArguments(
    output_dir="./snips_tok_clf/results",
    num_train_epochs=epochs,
    per_device_train_batch_size=32,
    per_device_eval_batch_size=32,
    load_best_model_at_end=True,        
    logging_steps=10,
    log_level='info',
    evaluation_strategy='epoch',
    save_strategy='epoch'
)

# Define the trainer:
trainer = Trainer(
         model=tok_clf_model,
         args=training_args,
         train_dataset=tok_clf_tokenized_snips['train'],
         eval_dataset=tok_clf_tokenized_snips['test'],
         data_collator=tok_data_collator
)

trainer.train()
trainer.evaluate()
```
