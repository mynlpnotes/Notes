# 6.4 BERT for QA

**Extractive Answering:**

* The answer to a question given a piece of context is direct substring of the context
* BERT and flavors of BERT

***

**Abstractive Answering:**

* The answer to a question given a piece of context is a free-form phrase based on the context
* Requires decoders
* GPT family, T5

```python
from transformers import BertTokenizerFast, BertForQuestionAnswering, pipeline, \
                         DataCollatorWithPadding, TrainingArguments, Trainer, \
                         AutoModelForQuestionAnswering, AutoTokenizer

bert_tokenizer = BertTokenizerFast.from_pretrained('bert-large-uncased', return_token_type_ids=True)
qa_bert = BertForQuestionAnswering.from_pretrained('bert-large-uncased')

qa_df = pd.read_csv('../data/qa.csv')
# This will have columns â€“ question, context, start_positions, end_positions, answer

# standard preprocessing here with truncation on to truncate longer text
def preprocess(data):
    return bert_tokenizer(data['question'], data['context'], truncation=True)
	
qa_dataset = qa_dataset.map(preprocess, batched=True)
# freeze all but the last 2 encoder layers in BERT to speed up training
for name, param in qa_bert.bert.named_parameters():
    if 'encoder.layer.22' in name:
        break
    param.requires_grad = False  # disable training in BERT

data_collator = DataCollatorWithPadding(tokenizer=bert_tokenizer)
trainer.train()

# Pretrained model
squad_pipe = pipeline("question-answering", "bert-large-uncased-whole-word-masking-finetuned-squad")

```
