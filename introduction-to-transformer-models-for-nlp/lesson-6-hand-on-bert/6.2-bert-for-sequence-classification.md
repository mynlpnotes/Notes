# 6.2 BERT for Sequence Classification

```python
from transformers import Trainer, TrainingArguments, DistilBertForSequenceClassification, DistilBertTokenizerFast, \
DataCollatorWithPadding, pipeline
from datasets import load_metric, Dataset
import numpy as np
from sklearn.preprocessing import LabelEncoder

snips_file = open('../data/snips.train.txt', 'rb')
# O - Entity meaning nothing
# B - Begining of an entity
# I - continuation of entity
[b'listen O\r\n',
 b'to O\r\n',
 b'westbam B-artist\r\n',
 b'alumb O\r\n',
 b'allergic B-album\r\n',â€¦]
	
# We do manupilation and bring it to this form
print(tokenized_utterances[0])
print(labels_for_tokens[0])
print([unique_token_labels[l] for l in labels_for_tokens[0]])
print(utterances[0])
print(sequence_labels[0])
print(unique_sequence_labels[sequence_labels[0]])
# ['listen', 'to', 'westbam', 'alumb', 'allergic', 'on', 'google', 'music']
#	[22, 22, 68, 22, 58, 22, 51, 4]
#	['O', 'O', 'B-artist', 'O', 'B-album', 'O', 'B-service', 'I-service']
#	
tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-cased')

snips_dataset = Dataset.from_dict(
		dict( utterance=utterances, label=sequence_labels,
     	        tokens=tokenized_utterances, token_labels=labels_for_tokens ))
snips_dataset = snips_dataset.train_test_split(test_size=0.2)
# {'utterance': 'rate homicide: a year on the killing streets five stars',
# 'label': 1,
# 'tokens': ['rate',
# 'homicide:',
# 'a',
# 'year',
# 'on',
# 'the',
# 'killing',
# 'streets',
# 'stars'],
# 'token_labels': [22, 44, 13, 13, 13, 13, 13, 13, 53, 29]}

def preprocess_function(examples):
	return tokenizer(examples["utterance"], truncation=True)
seq_clf_tokenized_snips = snips_dataset.map(preprocess_function, batched=True)
seq_clf_tokenized_snips['train'][0]
# Output:
# {'utterance': 'play a twenties song',
# 'label': 0,
# 'tokens': ['play', 'a', 'twenties', 'song'],
# 'token_labels': [22, 22, 37, 7],
# 'input_ids': [101, 1505, 170, 21708, 1461, 102],
# 'attention_mask': [1, 1, 1, 1, 1, 1]

# DataCollatorWithPadding creates batch of data. It also dynamically pads text to the 
#  length of the longest element in the batch, making them all the same length. 
#  It's possible to pad your text in the tokenizer function with padding=True, dynamic padding is more efficient.
	
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
sequence_clf_model = DistilBertForSequenceClassification.from_pretrained(
	    'distilbert-base-cased', 
	    num_labels=len(unique_sequence_labels),
	)
	
# set an index -> label dictionary
sequence_clf_model.config.id2label = {i: l for i, l in enumerate(unique_sequence_labels)}
metric = load_metric("accuracy")
	
def compute_metrics(eval_pred):  # custom method to take in logits and calculate accuracy of the eval set
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    return metric.compute(predictions=predictions, references=labels)

epochs = 2
	
training_args = TrainingArguments(
    output_dir="./snips_clf/results",
    num_train_epochs=epochs,
    per_device_train_batch_size=32,
    per_device_eval_batch_size=32,
    load_best_model_at_end=True,	    
# some deep learning parameters that the Trainer is able to take in
    warmup_steps=len(seq_clf_tokenized_snips['train']) // 5,  # number of warmup steps for learning rate scheduler,
    weight_decay = 0.05,
    logging_steps=1,
    log_level='info',
    evaluation_strategy='epoch',
    eval_steps=50,
    save_strategy='epoch'
)
	
# Define the trainer:
trainer = Trainer(
    model=sequence_clf_model,
    args=training_args,
    train_dataset=seq_clf_tokenized_snips['train'],
    eval_dataset=seq_clf_tokenized_snips['test'],
    compute_metrics=compute_metrics,
    data_collator=data_collator
)

trainer.train()
trainer.evaluate()
# {'eval_loss': 0.05155394226312637,
#  'eval_accuracy': 0.9870080244554834,
#  'eval_runtime': 51.0558,
#  'eval_samples_per_second': 51.258,
#  'eval_steps_per_second': 1.606,
#  'epoch': 2.0}

pipe = pipeline("text-classification", sequence_clf_model, tokenizer=tokenizer)
pipe('Please add Here We Go by Dispatch to my road trip playlist')
#Output: [{'label': 'AddToPlaylist', 'score': 0.9956121444702148}]

trainer.save_model()
pipe = pipeline("text-classification", "./snips_clf/results", tokenizer=tokenizer)

#Freezing the layers of the BERT
frozen_sequence_clf_model = DistilBertForSequenceClassification.from_pretrained(
	    'distilbert-base-cased', 
	    num_labels=len(unique_sequence_labels),
	)
for param in frozen_sequence_clf_model.distilbert.parameters():
    param.requires_grad = False
```
