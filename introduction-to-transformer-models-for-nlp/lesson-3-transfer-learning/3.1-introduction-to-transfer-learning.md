# 3.1 Introduction to Transfer Learning

**Transfer Learning:**

* A model trained for one task is reused as the starting point for a model for a second task
* A model is pre-trained on an unlabelled text corpora on an unsupervised task that generally doesn’t have a ‘useful’ object
* It is just meant to learn language/context in general
* The model is then fine tuned(updated) on a specific NLP ‘downstream’ task using a labelled dataset that is usually quite small in comparison
* If we simply trained on the downstream smaller dataset without pre-training, we would never be able to achieve the same state of the art results

***

**Approach:**

1. **Select source model:**

* Choose a pre-trained source model generally from a repository of models. We use huggingface’s model repository

2. **Resuse and train model:**

* The pre-trained model is the starting point for a second related task and trained on data pertaining to the second task
*

    <figure><img src="../../.gitbook/assets/image (51).png" alt=""><figcaption></figcaption></figure>
*

    <figure><img src="../../.gitbook/assets/image (52).png" alt=""><figcaption></figcaption></figure>
* This concept of transfer learning gave rise to the massive popularity of pretrained models in the NLP community
* Models pre-trained on a variety of tasks/data make for powerful pretrained models that are ready to be fine tuned on a variety of downstream tasks

&#x20;

***

**Pre-training BERT corpus:**

* English Wikipedia(2.5B words)
* Book corpus(800M words)

***

**Fine tuning BERT Classification:**

*

    <figure><img src="../../.gitbook/assets/image (53).png" alt=""><figcaption></figcaption></figure>
* If we took a pre-trained BERT model, there are learned weights from the pre-training which are going to be fine tuned during training
* We will also be a adding a separate layer on top of the BERT that will be randomly initialized and its job is to perform specific given task

***

**Approached for fine tuning:**

1. Update the whole model on labelled data + any additional layers added on top

*

    <figure><img src="../../.gitbook/assets/image (54).png" alt=""><figcaption></figcaption></figure>

2. Freeze a subset of the model

*

    <figure><img src="../../.gitbook/assets/image (55).png" alt=""><figcaption></figcaption></figure>

3. Freeze the whole model and only train the additional layers added on top

*

    <figure><img src="../../.gitbook/assets/image (56).png" alt=""><figcaption></figcaption></figure>
* Suppose we are having finance use case, but BERT has learned Wikipedia model, so it wont understand the terms of finance good
* So better not to use this     &#x20;

&#x20;

&#x20;
