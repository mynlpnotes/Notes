# 2.3 Multi Headed Attention

* Multi headed self attention, is simply the idea that instead of passing through an input representation across an individual softmax
*

    <figure><img src="../../.gitbook/assets/image (9).png" alt=""><figcaption></figcaption></figure>
* Each encoder doesn’t have one attention mechanism, but it has several smaller attention mechanism(smaller embedding dimension) concatenated together
* So it means each encoder has same attention calculation happening side by side, but they don’t actually talk to each other
*

    <figure><img src="../../.gitbook/assets/image (1) (1).png" alt=""><figcaption></figcaption></figure>
* Here we are having 8 different set of Q,K and V and we end up getting 8 different results
* None of this talk to each other
* BERT base model has 12 attention heads and 12 encoded layers
* So there are 144 scaled dot product happening
* So each one might start understanding different grammatical rules
* So when we put them all together, it should have a generic understanding of language

&#x20;

***

**Transformers + BERT using multi headed self-attention. Why?**

1. BERT focuses on multiple relationships simultaneously

Example

* Sinan ate the cereal and then he had a stomach ache
* The word he refers to sinan as a pronoun
* The word he also relates to had

2. Helps BERT learn different types of relationships between words by transforming our inputs to multiple sub-spaces of Q/K/V

3\.     &#x20;

<figure><img src="../../.gitbook/assets/image (2) (1).png" alt=""><figcaption></figcaption></figure>

*

    <figure><img src="../../.gitbook/assets/image (3) (1).png" alt=""><figcaption></figcaption></figure>
* If we get the output of attention of BERT, then we get a square matrix
* Each row here adds upto 1
* If we take the row ‘me’, then we see it pays a lot of attention to . and \[SEP]
* We can use bertviz to visualize this

&#x20;
