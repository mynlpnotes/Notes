# 2.2 Scaled Dot Product Attention

* Standard attention is calculated using three matrices called query, key and value matrices
* X = Our tokenized input
* Each row is the token embedding for each token in the input
* The input has already gone through BERTs pre processing(token, segment and position)
* In this example, we have an input of 2 tokens
* The matrix has shape: number of tokens X token dimension
* Here token dimension is 4
* We perform 3 linear transformations
* The new vector spaces are called, query space, key space and value space
* We started with 4 dimension but then we got compressed it to 3
* We are doing this as we want to impart meaning to this matrices, and then allow a training phase to actually learn the proper weight matrices for Wq, Wk and Wv
* Query matrix is meant to represent an information that we are looking for, point which we are trying to make
* Key matrix is meant to represent relevance of each word to our query, and it represents how important each word is to the overall query that I have
* Value matrix represents the contextless meaning of our input tokens
* V is basically just a compressed version of X, contextless means pre-attention
* Eventually we want to end up with context representation
* The last step of scalar dot product makes V from contextless to contextfull
*

    <figure><img src="../../.gitbook/assets/image (42).png" alt=""><figcaption></figcaption></figure>
* When we calculate attention scores for out tokens, the goal is to look at each token, and its relevance to every other token in the sequence
* Here we are calculating attention score for word ‘like’
* Smaller values meant vectors were not close and therefore less attention
* Larger values meant vectors were close and therefore more attention
*

    <figure><img src="../../.gitbook/assets/image (43).png" alt=""><figcaption></figcaption></figure>
* So it means when we talk about like, we should pay 22% attention to I, 42% like and 36% to cats
* We multiply this attention scores by Context less V and get context full embedding
*

    <figure><img src="../../.gitbook/assets/image (44).png" alt=""><figcaption></figcaption></figure>
*

&#x20;

***

**Code:**

* from transformers import BertModel
* model = BertModel.from\_pretrained(‘bert-base-uncased’)
* len(model.encode.layer) -> 12
*   &#x20;  &#x20;

    <figure><img src="../../.gitbook/assets/image (47).png" alt=""><figcaption></figcaption></figure>
