# 1.3 Encoder and Decoder Architecture

**Transformers:**

* Current SOTA NLP model
* Relies entirely on self attention to model the relationship between tokens in a sentence rather than relying on recursion like RNNs and LSTMs do
*

    <figure><img src="../../.gitbook/assets/image (10).png" alt=""><figcaption></figcaption></figure>
* Consists of an encoder and decoder
* The encoder takes in our input and outputs a matrix representation of that input
* The decoder takes in that representation and iteratively generates an output
*

    <figure><img src="../../.gitbook/assets/image (9) (1).png" alt=""><figcaption></figcaption></figure>
* The encoder is actually an encoder stack with multiple encoders
* Stacked on top on one another
* Representation of 1st encoder goes to 2nd encoder and so on
* ![](<../../.gitbook/assets/image (11).png>)
* Decoder is also a stack of multiple decoders

&#x20;

***

**Original Architecture:**

*

    <figure><img src="../../.gitbook/assets/image (12).png" alt=""><figcaption></figcaption></figure>
* Input gets transformed and pre-processed into multiple input embeddings
* These embeddings are feeded to Nx encoder
* Number of Encoder and Decoder depends on Author and Difficulty of the problem
* Then this embeddings are feed into the decoder
* The way its fed to the decoder is by another type of attention called cross attention
* Cross attention is a special kind of attention that takes part of attention from encoder and part from decoder and then put its together
* This is how transformer relates what is being input and what is being output
* Through cross attention and through an embedding of the output, the decoder is able to put this together to be able to actually output probabilities
* The multi head attention allows the encoder to learn simultaneous structures and rules all at once
* Decoder has masked multi headed attention
* Combine multi headed attention and multi masked attention combined with cross attention, builds the idea of attention
