# 1.4 How Language Model Looks at text

**Language Model:**

* A model is trained to predict a missing word in a sequence of words

1. **Auto-regressive:**

* Goal is to predict a future token given either the past or future tokens, but not both
* Natural language generation
* GPT family

2. **Auto-encoding:**

* Goal is to learn representation of the entire sequence by predicting tokens given both the past and future tokens
* NLU
* BERT
* Example:
* If you donâ€™t \_\_\_ at the sign, you will get a ticket
* This is a language modelling task
* Answer can be stop/yield, both of which are correct, this is an auto encoding task
