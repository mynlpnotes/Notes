# 1.2 Pay Attention with Attention

**Attention in Neural Networks:**

* The transformer model is based primarily on the idea of attention – a family of mechanisms designed to “attend to” parts of a sequence in the context of another sequence usually for the purpose of some prediction task

&#x20;

***

**Image captioning problem:**

* ![](<../../.gitbook/assets/image (5) (1).png>)

&#x20;

***

**Language Translation:**

* When translating English to French, we see that the French word marin is paying attention to the English word marine even though they don’t occur at the same index

\-            ![](<../../.gitbook/assets/image (6) (1).png>)

&#x20;

***

**Self-Attention:**

* Consider the following phrase:
* My friend told me about this class and I love it so far! She was right
* The pronoun ‘I’ refers to the antecedent ‘me’
* The pronoun ‘She’ refers to the antecedent ‘friend’
* The pronoun ‘I’ also refers to the fact that I ‘love’ the class
* This is what we need to teach our architecture
* With self attention, we alter the idea of attention to relate each word in the phrase with every other word in the phrase to teach the model the relationships between them
* This are the ouput of BERT models attention scores:
*

    <figure><img src="../../.gitbook/assets/image (7) (1).png" alt=""><figcaption></figcaption></figure>
* This is bert showing that it understand the relationship between I and me
* Similarly for she and friend
* Was right is also highlighted showing that bert also understand that she was right

&#x20;

&#x20;

&#x20;

&#x20;
