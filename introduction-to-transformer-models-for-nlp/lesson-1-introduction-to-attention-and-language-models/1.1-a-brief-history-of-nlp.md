# 1.1 A Brief History of NLP



***

**2001:**

* The first neural language model was able to predict future words in a sentence using feed forward neural network



***

**2013:**

* Two methods for the “Word2Vec” task – learning semantic word embeddings and association in a vector space – CBOW and Skip-gram
*   &#x20;     &#x20;

    <figure><img src="../../.gitbook/assets/image (4) (1).png" alt=""><figcaption></figcaption></figure>
* Also came to know about the bias shown by this models
* ![](<../../.gitbook/assets/image (1) (1) (1).png>)

&#x20;

***

**2013:**

* RNNs and CNNs for NLP
* Vanilla RNN were introduced in 1990 and LSTM in 1997
* Training RNN were thought to be difficult until a PhD thesis in 2013 by Ilya Sutskever



***

**2014:**

* Sequence to Sequence Modelling
* An encoder reads a variable length input sequence and a decoder produces a variable length sequence output using a fixed length hidden state from the encoder
* ![](<../../.gitbook/assets/image (2) (1) (1).png>)

&#x20;

***

**2015:**

* Compressing a sequence into a fixed length vector makes it hard to remember long term information.
* Attention allows the decoder access to the encoder hidden states, which are provided as additional inputs to the decoder
* ![](<../../.gitbook/assets/image (3) (1) (1).png>)



***

**2017:**

* Attention is all you need
* Postulated that attention could be the primary mechanism powering a language model to replace slow recurrent networks

&#x20;

***

**2018:**

* Pretrained Language Models
* Using models pretrained on massive corpora to learn general language rules like BERT, GPT, T5 and ELMO as a starting point showed improvements in nearly all areas of NLP compared to previous state of the art results
*   &#x20;     &#x20;

    <figure><img src="../../.gitbook/assets/image (4) (1) (1).png" alt=""><figcaption></figcaption></figure>

&#x20;

&#x20;

&#x20;
