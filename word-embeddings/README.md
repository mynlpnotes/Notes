# Word Embeddings

* Techniques which converts words into numbers/vectors
* 2 Types

1. Based on count or frequency - BOW, TF-IDF, One hot encoding
2. Deep learning trained model - Word2Vec - CBOW, SkipGrams



*

    <figure><img src="../.gitbook/assets/image (20).png" alt=""><figcaption></figcaption></figure>
* Represent words as vectors
* Pen and pencil can be grouped together as they are near
* Apple is far from them
* To find how much pen and pencil are similar, we will find the angle between them
* For book written on story and on mathematics, their word embeddings will be different. For e.g., python can have different meaning in both the books
